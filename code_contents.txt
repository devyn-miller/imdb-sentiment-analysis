anyio==4.2.0
argon2-cffi==23.1.0
argon2-cffi-bindings==21.2.0
arrow==1.3.0
asttokens==2.4.1
async-lru==2.0.4
attrs==23.2.0
Babel==2.14.0
beautifulsoup4==4.12.3
bleach==6.1.0
certifi==2024.2.2
cffi==1.16.0
charset-normalizer==3.3.2
colorama==0.4.6
comm==0.2.1
contourpy==1.2.0
cycler==0.12.1
debugpy==1.8.0
decorator==5.1.1
defusedxml==0.7.1
exceptiongroup==1.2.0
executing==2.0.1
fastjsonschema==2.19.1
filelock==3.13.1
fonttools==4.48.1
fqdn==1.5.1
fsspec==2024.2.0
gitdb==4.0.11
GitPython==3.1.41
h11==0.14.0
httpcore==1.0.2
httpx==0.26.0
idna==3.6
ipykernel==6.29.1
ipython==8.21.0
isoduration==20.11.0
jedi==0.19.1
Jinja2==3.1.3
joblib==1.3.2
json5==0.9.14
jsonpointer==2.4
jsonschema==4.21.1
jsonschema-specifications==2023.12.1
jupyter-events==0.9.0
jupyter-lsp==2.2.2
jupyter-server-mathjax==0.2.6
jupyter_client==8.6.0
jupyter_core==5.7.1
jupyter_server==2.12.5
jupyter_server_terminals==0.5.2
jupyterlab==4.1.0
jupyterlab_git==0.50.0
jupyterlab_pygments==0.3.0
jupyterlab_server==2.25.2
kiwisolver==1.4.5
MarkupSafe==2.1.5
matplotlib==3.8.2
matplotlib-inline==0.1.6
mistune==3.0.2
mpmath==1.3.0
nbclient==0.9.0
nbconvert==7.15.0
nbdime==4.0.1
nbformat==5.9.2
nest-asyncio==1.6.0
networkx==3.2.1
notebook_shim==0.2.3
numpy==1.26.4
nvidia-cublas-cu12==12.1.3.1
nvidia-cuda-cupti-cu12==12.1.105
nvidia-cuda-nvrtc-cu12==12.1.105
nvidia-cuda-runtime-cu12==12.1.105
nvidia-cudnn-cu12==8.9.2.26
nvidia-cufft-cu12==11.0.2.54
nvidia-curand-cu12==10.3.2.106
nvidia-cusolver-cu12==11.4.5.107
nvidia-cusparse-cu12==12.1.0.106
nvidia-nccl-cu12==2.19.3
nvidia-nvjitlink-cu12==12.3.101
nvidia-nvtx-cu12==12.1.105
overrides==7.7.0
packaging==23.2
pandas==2.2.0
pandocfilters==1.5.1
parso==0.8.3
pexpect==4.9.0
pillow==10.2.0
platformdirs==4.2.0
plotly==5.18.0
prometheus-client==0.19.0
prompt-toolkit==3.0.43
psutil==5.9.8
ptyprocess==0.7.0
pure-eval==0.2.2
pycparser==2.21
Pygments==2.17.2
pyparsing==3.1.1
python-dateutil==2.8.2
python-json-logger==2.0.7
pytz==2024.1
PyYAML==6.0.1
pyzmq==25.1.2
referencing==0.33.0
requests==2.31.0
rfc3339-validator==0.1.4
rfc3986-validator==0.1.1
rpds-py==0.17.1
scikit-learn==1.4.0
scipy==1.12.0
seaborn==0.13.2
Send2Trash==1.8.2
six==1.16.0
smmap==5.0.1
sniffio==1.3.0
soupsieve==2.5
stack-data==0.6.3
sympy==1.12
tenacity==8.2.3
terminado==0.18.0
threadpoolctl==3.2.0
tinycss2==1.2.1
tomli==2.0.1
torch==2.2.0
tornado==6.4
traitlets==5.14.1
triton==2.2.0
types-python-dateutil==2.8.19.20240106
typing_extensions==4.9.0
tzdata==2023.4
uri-template==1.3.0
urllib3==2.0.7
wcwidth==0.2.13
webcolors==1.13
webencodings==0.5.1
websocket-client==1.7.0


from preprocess import load_and_preprocess_data
from transformers import TFBertForSequenceClassification, BertTokenizer
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

train_encodings, train_labels, test_encodings, test_labels = load_and_preprocess_data()

# Convert to TensorFlow datasets for easier training
train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_labels))
test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), test_labels))


model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

optimizer = Adam(learning_rate=5e-5)
model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy'])

early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

model.fit(train_encodings, train_labels, epochs=10, batch_size=8, validation_data=(test_encodings, test_labels), callbacks=[early_stopping])


from transformers import BertTokenizer
from tensorflow.keras.utils import to_categorical
import tensorflow_datasets as tfds

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

def encode_reviews(reviews, max_length=512):
    return tokenizer(reviews, padding=True, truncation=True, max_length=max_length, return_tensors="tf")

def load_and_preprocess_data(max_length=512):
    train_data, test_data = tfds.load(name="imdb_reviews", split=('train', 'test'), as_supervised=True)
    train_reviews, train_labels = [], []
    test_reviews, test_labels = [], []

    for review, label in tfds.as_numpy(train_data):
        train_reviews.append(review.decode('utf-8'))
        train_labels.append(label)

    for review, label in tfds.as_numpy(test_data):
        test_reviews.append(review.decode('utf-8'))
        test_labels.append(label)

    train_encodings = encode_reviews(train_reviews, max_length)
    test_encodings = encode_reviews(test_reviews, max_length)

    train_labels = to_categorical(train_labels, num_classes=2)
    test_labels = to_categorical(test_labels, num_classes=2)

    return train_encodings, train_labels, test_encodings, test_labels

from transformers import BertTokenizer, TFBertForSequenceClassification
import tensorflow as tf
import numpy as np

model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

def predict_sentiment(review):
    inputs = tokenizer(review, return_tensors="tf", truncation=True, padding=True, max_length=512)
    outputs = model(inputs)
    prediction = tf.nn.softmax(outputs.logits, axis=-1)
    labels = ['Negative', 'Positive']
    predicted_label = labels[np.argmax(prediction)]
    return predicted_label


import gradio as gr
from predict import predict_sentiment

def predict_and_visualize(review):
    sentiment = predict_sentiment(review)
    return sentiment

iface = gr.Interface(
    fn=predict_and_visualize,
    inputs=gr.inputs.Textbox(lines=2, placeholder="Type your review here..."),
    outputs="text",
    title="Sentiment Analysis Tool",
    description="Enter a movie review to determine its sentiment. Positive or Negative.",
    examples=[["I loved this movie, it was fantastic!"], ["I hated this movie, it was terrible."]]
)

iface.launch(share=True)


